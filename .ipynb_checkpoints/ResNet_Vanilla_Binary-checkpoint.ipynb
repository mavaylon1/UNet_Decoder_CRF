{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras_segmentation.models import resnet50 as rs\n",
    "# from keras_segmentation.models.unet import resnet50_unet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm \n",
    "#import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from crfrnn_layer import CrfRnnLayer\n",
    "from keras.models import Model\n",
    "from image_segmentation_keras.keras_segmentation.models.model_utils import get_segmentation_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, ZeroPadding2D, \\\n",
    "    Dropout, Conv2DTranspose, Cropping2D, Add, UpSampling2D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import glob\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras import backend as keras\n",
    "from keras import Model\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels, height, width = 3, 512, 512\n",
    "input_height=256 #416\n",
    "input_width=256 #608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unet(n_classes, encoder, l1_skip_conn=True, input_height=416,\n",
    "          input_width=608):\n",
    "\n",
    "  \n",
    "    img_input, levels = encoder(\n",
    "        input_height=input_height, input_width=input_width)\n",
    "    [f1, f2, f3, f4, f5] = levels\n",
    "    \n",
    "    print(\"f5\",f5.shape)\n",
    "\n",
    "    o = f5\n",
    "#     o = (UpSampling2D((2, 2), data_format=IMAGE_ORDERING))(o)\n",
    "\n",
    "    \n",
    "#     o = (Conv2D(512, (3, 3), padding='same' , activation='relu' , data_format=IMAGE_ORDERING))(o)\n",
    "#     o = (Conv2D(512, (3, 3), padding='same' , activation='relu' , data_format=IMAGE_ORDERING))(o)\n",
    "    o = (BatchNormalization())(o)\n",
    "    o = Conv2DTranspose(1024,(2,2),strides=2,padding='same')(o)\n",
    "    print('first upsample',o.shape)\n",
    "\n",
    "    o = (concatenate([o, f4], axis=3))\n",
    "    print(\"f4,first conc\",o.shape)\n",
    "    o = (Conv2D(1024, (3, 3), padding='same', activation='relu' , data_format=IMAGE_ORDERING))(o)\n",
    "    o = (Conv2D(1024, (3, 3), padding='same', activation='relu' , data_format=IMAGE_ORDERING))(o)\n",
    "    print(\"f4 after double conv\",o.shape)\n",
    "    o = (BatchNormalization())(o)\n",
    "    \n",
    "\n",
    "    o = Conv2DTranspose(512,(2,2),strides=2,padding='same')(o)\n",
    "    print('second upsample',o.shape)\n",
    "    o = (concatenate([o, f3], axis=3))\n",
    "    print(\"f3/second conc\",o.shape)\n",
    "    o = (Conv2D(512, (3, 3), padding='same' , activation='relu' , data_format=IMAGE_ORDERING))(o)\n",
    "    o = (Conv2D(512, (3, 3), padding='same' , activation='relu' , data_format=IMAGE_ORDERING))(o)\n",
    "    o = (BatchNormalization())(o)\n",
    "    \n",
    "\n",
    "    o = Conv2DTranspose(256,(2,2),strides=2,padding='same')(o)\n",
    "    print('third upsample',o.shape)\n",
    "    o = (concatenate([o, f2], axis=3))\n",
    "    print('third concat')\n",
    "    \n",
    "    o = (Conv2D(256, (3, 3), padding='same', activation='relu', data_format=IMAGE_ORDERING))(o)\n",
    "    o = (Conv2D(256, (3, 3), padding='same', activation='relu', data_format=IMAGE_ORDERING))(o)\n",
    "    o = (BatchNormalization())(o)\n",
    "\n",
    "    o = Conv2DTranspose(128,(2,2),strides=2,padding='same')(o)\n",
    "    print('fourth upsample',o.shape)\n",
    "    o = (concatenate([o, f1], axis=3))\n",
    "    print('fourth concat',o.shape)\n",
    "\n",
    "    o = (Conv2D(128, (3, 3), padding='same', activation='relu', data_format=IMAGE_ORDERING))(o)\n",
    "    o = (Conv2D(128, (3, 3), padding='same', activation='relu', data_format=IMAGE_ORDERING))(o) \n",
    "    o = (BatchNormalization())(o)\n",
    "    \n",
    "    o = Conv2DTranspose(64,(2,2),strides=2,padding='same')(o)\n",
    "    print('fifth upsample',o.shape)\n",
    "    o = (Conv2D(64, (3, 3), padding='same', activation='relu', data_format=IMAGE_ORDERING))(o)\n",
    "    o = (Conv2D(64, (3, 3), padding='same', activation='relu', data_format=IMAGE_ORDERING))(o)\n",
    "    print(\"last double conv\",o.shape)\n",
    "    o = Conv2D(n_classes, (3, 3), padding='same',\n",
    "               data_format=IMAGE_ORDERING)(o)\n",
    "    print(\"cnn output\",o.shape)\n",
    "\n",
    "    model = get_segmentation_model(img_input, o)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras import layers\n",
    "\n",
    "# Source:\n",
    "# https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py\n",
    "\n",
    "\n",
    "from keras_segmentation.models.config import IMAGE_ORDERING\n",
    "\n",
    "if IMAGE_ORDERING == 'channels_first':\n",
    "    MERGE_AXIS = 1\n",
    "elif IMAGE_ORDERING == 'channels_last':\n",
    "    MERGE_AXIS = -1\n",
    "\n",
    "if IMAGE_ORDERING == 'channels_first':\n",
    "    pretrained_url = \"https://github.com/fchollet/deep-learning-models/\" \\\n",
    "                     \"releases/download/v0.2/\" \\\n",
    "                     \"resnet50_weights_th_dim_ordering_th_kernels_notop.h5\"\n",
    "elif IMAGE_ORDERING == 'channels_last':\n",
    "    pretrained_url = \"https://github.com/fchollet/deep-learning-models/\" \\\n",
    "                     \"releases/download/v0.2/\" \\\n",
    "                     \"resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "\n",
    "\n",
    "def one_side_pad(x):\n",
    "    x = ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING)(x)\n",
    "    if IMAGE_ORDERING == 'channels_first':\n",
    "        x = Lambda(lambda x: x[:, :, :-1, :-1])(x)\n",
    "    elif IMAGE_ORDERING == 'channels_last':\n",
    "        x = Lambda(lambda x: x[:, :-1, :-1, :])(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: defualt 3, the kernel size of middle conv layer at\n",
    "                     main path\n",
    "        filters: list of integers, the filterss of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    if IMAGE_ORDERING == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), data_format=IMAGE_ORDERING,\n",
    "               name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size, data_format=IMAGE_ORDERING,\n",
    "               padding='same', name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1), data_format=IMAGE_ORDERING,\n",
    "               name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block,\n",
    "               strides=(2, 2)):\n",
    "    \"\"\"conv_block is the block that has a conv layer at shortcut\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: defualt 3, the kernel size of middle conv layer at\n",
    "                     main path\n",
    "        filters: list of integers, the filterss of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    Note that from stage 3, the first conv layer at main path is with\n",
    "    strides=(2,2) and the shortcut should have strides=(2,2) as well\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    if IMAGE_ORDERING == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), data_format=IMAGE_ORDERING, strides=strides,\n",
    "               name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size, data_format=IMAGE_ORDERING,\n",
    "               padding='same', name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1), data_format=IMAGE_ORDERING,\n",
    "               name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    shortcut = Conv2D(filters3, (1, 1), data_format=IMAGE_ORDERING,\n",
    "                      strides=strides, name=conv_name_base + '1')(input_tensor)\n",
    "    shortcut = BatchNormalization(\n",
    "        axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
    "#     print(shortcut.shape)\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_resnet50_encoder(input_height,  input_width,\n",
    "                        \n",
    "                         include_top=True, \n",
    "                         input_tensor=None, input_shape=None,\n",
    "                         pooling=None,\n",
    "                         classes=1000):\n",
    "    print(input_height)\n",
    "    assert input_height % 32 == 0\n",
    "    assert input_width % 32 == 0\n",
    "\n",
    "    if IMAGE_ORDERING == 'channels_first':\n",
    "        img_input = Input(shape=(3, input_height, input_width))\n",
    "    elif IMAGE_ORDERING == 'channels_last':\n",
    "        img_input = Input(shape=(input_height, input_width, 3))\n",
    "\n",
    "    if IMAGE_ORDERING == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "\n",
    "    x = ZeroPadding2D((3, 3), data_format=IMAGE_ORDERING)(img_input)\n",
    "    x = Conv2D(64, (7, 7), data_format=IMAGE_ORDERING,\n",
    "               strides=(2, 2), name='conv1')(x)\n",
    "    f1 = x\n",
    "\n",
    "    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((3, 3), data_format=IMAGE_ORDERING, strides=(2, 2))(x)\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "    f2 = one_side_pad(x)\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
    "    f3 = x\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
    "    f4 = x\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
    "    f5 = x\n",
    "\n",
    "    x = AveragePooling2D(\n",
    "        (7, 7), data_format=IMAGE_ORDERING, name='avg_pool')(x)\n",
    "    # f6 = x\n",
    "\n",
    "#     if pretrained == 'imagenet':\n",
    "#         weights_path = keras.utils.get_file(\n",
    "#             pretrained_url.split(\"/\")[-1], pretrained_url)\n",
    "#         Model(img_input, x).load_weights(weights_path)\n",
    "\n",
    "    return img_input, [f1, f2, f3, f4, f5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet50_unet(n_classes, input_height=512, input_width=512,\n",
    "                  encoder_level=3):\n",
    "\n",
    "    model = _unet(n_classes, get_resnet50_encoder,\n",
    "                  input_height=input_height, input_width=input_width)\n",
    "    model.model_name = \"resnet50_unet\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "f5 (None, 8, 8, 2048)\n",
      "first upsample (None, None, None, 1024)\n",
      "f4,first conc (None, 16, 16, 2048)\n",
      "f4 after double conv (None, 16, 16, 1024)\n",
      "second upsample (None, None, None, 512)\n",
      "f3/second conc (None, 32, 32, 1024)\n",
      "third upsample (None, None, None, 256)\n",
      "third concat\n",
      "fourth upsample (None, None, None, 128)\n",
      "fourth concat (None, 128, 128, 192)\n",
      "fifth upsample (None, None, None, 64)\n",
      "last double conv (None, None, None, 64)\n",
      "cnn output (None, None, None, 3)\n"
     ]
    }
   ],
   "source": [
    "model = resnet50_unet(n_classes=2 ,  input_height=512, input_width=512  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5912 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5912/5912 [00:17<00:00, 332.05it/s]\n",
      "  2%|▏         | 32/1478 [00:00<00:04, 315.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset verified! \n",
      "Verifying validation dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1478/1478 [00:04<00:00, 338.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset verified! \n",
      "correct\n",
      "Epoch 1/20\n",
      " 147/5912 [..............................] - ETA: 3:27:06 - loss: 0.8919 - accuracy: 0.6292"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4d310e9e264d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mval_annotations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/Users/mavaylon/Research/image-segmentation-keras/Data/test/ann/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mval_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mval_steps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/mavaylon/Research/image-segmentation-keras/Data/test/img/*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;32m~/Research/LBNL_Segmentation_crf/image_segmentation_keras/keras_segmentation/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_images, train_annotations, input_height, input_width, n_classes, verify_dataset, checkpoints_path, epochs, batch_size, validate, val_images, val_annotations, val_batch_size, auto_resume_checkpoint, load_weights, steps_per_epoch, val_steps_per_epoch, gen_use_multiprocessing, ignore_zero_class, optimizer_name, do_augment, augmentation_name)\u001b[0m\n\u001b[1;32m    165\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_steps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                             use_multiprocessing=gen_use_multiprocessing)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    train_images = \"/Users/mavaylon/Research/Research_Gambier/Data/BP_C_train/img/\",\n",
    "    train_annotations = \"/Users/mavaylon/Research/Research_Gambier/Data/BP_C_train/ann/\",\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(glob(\"/Users/mavaylon/Research/Research_Gambier/Data/BP_C_train/img/*\")),\n",
    "    batch_size=1,\n",
    "    validate=True,\n",
    "    val_images=\"/Users/mavaylon/Research/Research_Gambier/Data/BP_C_test/img/\",\n",
    "    val_annotations=\"/Users/mavaylon/Research/Research_Gambier/Data/BP_C_test/ann/\",\n",
    "    val_batch_size=1,\n",
    "    val_steps_per_epoch=len(glob(\"/Users/mavaylon/Research/image-segmentation-keras/Data/test/img/*\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights(\"bp_castle_resnet_model_with_crfrnn_6epoch.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"/Users/mavaylon/Research/image-segmentation-keras/pet_class_crf.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "img_names = sorted(glob.glob(\"/Users/mavaylon/Research/image-segmentation-keras/images_png/*.png\"))\n",
    "\n",
    "for name in img_names:\n",
    "    out_name = \"/Users/mavaylon/Research/image-segmentation-keras/pet_test/\" + name.split('/')[-1]\n",
    "    print(out_name)\n",
    "    out = model.predict_segmentation(inp=name, out_fname=out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=cv2.imread('/Users/mavaylon/Research/image-segmentation-keras/pet_test/Abyssinian_1.png',0)\n",
    "plt.imshow(test)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images51=sorted(glob(\"/Users/mavaylon/Research/image-segmentation-keras/dataset1/images_prepped_train/*\"))\n",
    "ann51=sorted(glob('/Users/mavaylon/Research/image-segmentation-keras/dataset1/annotations_prepped_train/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img51=cv2.imread(images51[0],cv2.IMREAD_COLOR)\n",
    "plt.imshow(img51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask51=cv2.imread(ann51[0],1)\n",
    "plt.imshow(mask51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_mask=cv2.imread('/Users/mavaylon/Research/image-segmentation-keras/oxford-iiit-pet/annotations/trimaps/Abyssinian_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=sorted(glob('/Users/mavaylon/Research/image-segmentation-keras/oxford-iiit-pet/annotations/trimaps/*'))\n",
    "pet_shape=[]\n",
    "for i in f:\n",
    "    img=cv2.imread(i,1)\n",
    "    pet_shape.append(img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(pet_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_path='/Users/mavaylon/Research/image-segmentation-keras/oxford-iiit-pet/annotations/trimaps/Abyssinian_1.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot some samples\n",
    "rows,cols=2,2\n",
    "fig=plt.figure(figsize=(10,10))\n",
    "for i in range(1,rows*cols+1):\n",
    "    fig.add_subplot(rows,cols,i)\n",
    "    img_path=images51[i]\n",
    "    msk_path=ann51[i]\n",
    "    img=cv2.imread(img_path)\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    msk=cv2.imread(msk_path)\n",
    "    plt.imshow(img)\n",
    "    plt.title(i)\n",
    "    plt.imshow(msk,alpha=0.4)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmentation_array(image_input, nClasses,\n",
    "                           width, height, no_reshape=False):\n",
    "    \"\"\" Load segmentation array from input \"\"\"\n",
    "\n",
    "    seg_labels = np.zeros((height, width, nClasses))\n",
    "\n",
    "#     if type(image_input) is np.ndarray:\n",
    "#         # It is already an array, use it as it is\n",
    "#         img = image_input\n",
    "#     elif isinstance(image_input, six.string_types):\n",
    "#         if not os.path.isfile(image_input):\n",
    "#             raise DataLoaderError(\"get_segmentation_array: \"\n",
    "#                                   \"path {0} doesn't exist\".format(image_input))\n",
    "    img = cv2.imread(image_input, 1)\n",
    "#     else:\n",
    "#         raise DataLoaderError(\"get_segmentation_array: \"\n",
    "#                               \"Can't process input type {0}\"\n",
    "#                               .format(str(type(image_input))))\n",
    "\n",
    "    img = cv2.resize(img, (width, height), interpolation=cv2.INTER_NEAREST)\n",
    "    img = img[:, :, 0]\n",
    "\n",
    "    for c in range(nClasses):\n",
    "        seg_labels[:, :, c] = (img == c).astype(int)\n",
    "\n",
    "    if not no_reshape:\n",
    "        seg_labels = np.reshape(seg_labels, (width*height, nClasses))\n",
    "\n",
    "    return seg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_segmentation_array(pet_path,3,400,300).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_array(image_input,\n",
    "                    width, height,\n",
    "                    imgNorm=\"sub_mean\", ordering='channels_first'):\n",
    "    \"\"\" Load image array from input \"\"\"\n",
    "\n",
    "#     if type(image_input) is np.ndarray:\n",
    "#         # It is already an array, use it as it is\n",
    "#         img = image_input\n",
    "#     elif isinstance(image_input, six.string_types):\n",
    "#         if not os.path.isfile(image_input):\n",
    "#             raise DataLoaderError(\"get_image_array: path {0} doesn't exist\"\n",
    "#                                   .format(image_input))\n",
    "    img = cv2.imread(image_input, 1)\n",
    "#     else:\n",
    "#         raise DataLoaderError(\"get_image_array: Can't process input type {0}\"\n",
    "#                               .format(str(type(image_input))))\n",
    "\n",
    "    if imgNorm == \"sub_and_divide\":\n",
    "        img = np.float32(cv2.resize(img, (width, height))) / 127.5 - 1\n",
    "    elif imgNorm == \"sub_mean\":\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        img = img.astype(np.float32)\n",
    "        img[:, :, 0] -= 103.939\n",
    "        img[:, :, 1] -= 116.779\n",
    "        img[:, :, 2] -= 123.68\n",
    "        img = img[:, :, ::-1]\n",
    "    elif imgNorm == \"divide\":\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        img = img.astype(np.float32)\n",
    "        img = img/255.0\n",
    "\n",
    "    if ordering == 'channels_first':\n",
    "        img = np.rollaxis(img, 2, 0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_image_array(images51[0],608,416).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs_from_paths(images_path, segs_path, ignore_non_matching=False):\n",
    "    \"\"\" Find all the images from the images_path directory and\n",
    "        the segmentation images from the segs_path directory\n",
    "        while checking integrity of data \"\"\"\n",
    "\n",
    "    ACCEPTABLE_IMAGE_FORMATS = [\".jpg\", \".jpeg\", \".png\", \".bmp\"]\n",
    "    ACCEPTABLE_SEGMENTATION_FORMATS = [\".png\", \".bmp\"]\n",
    "\n",
    "    image_files = []\n",
    "    segmentation_files = {}\n",
    "\n",
    "    for dir_entry in os.listdir(images_path):\n",
    "        if os.path.isfile(os.path.join(images_path, dir_entry)) and \\\n",
    "                os.path.splitext(dir_entry)[1] in ACCEPTABLE_IMAGE_FORMATS:\n",
    "            file_name, file_extension = os.path.splitext(dir_entry)\n",
    "            image_files.append((file_name, file_extension,\n",
    "                                os.path.join(images_path, dir_entry)))\n",
    "\n",
    "    for dir_entry in os.listdir(segs_path):\n",
    "        if os.path.isfile(os.path.join(segs_path, dir_entry)) and \\\n",
    "           os.path.splitext(dir_entry)[1] in ACCEPTABLE_SEGMENTATION_FORMATS:\n",
    "            file_name, file_extension = os.path.splitext(dir_entry)\n",
    "            full_dir_entry = os.path.join(segs_path, dir_entry)\n",
    "            if file_name in segmentation_files:\n",
    "                raise DataLoaderError(\"Segmentation file with filename {0}\"\n",
    "                                      \" already exists and is ambiguous to\"\n",
    "                                      \" resolve with path {1}.\"\n",
    "                                      \" Please remove or rename the latter.\"\n",
    "                                      .format(file_name, full_dir_entry))\n",
    "\n",
    "            segmentation_files[file_name] = (file_extension, full_dir_entry)\n",
    "\n",
    "    return_value = []\n",
    "    # Match the images and segmentations\n",
    "    for image_file, _, image_full_path in image_files:\n",
    "        if image_file in segmentation_files:\n",
    "            return_value.append((image_full_path,\n",
    "                                segmentation_files[image_file][1]))\n",
    "        elif ignore_non_matching:\n",
    "            continue\n",
    "        else:\n",
    "            # Error out\n",
    "            raise DataLoaderError(\"No corresponding segmentation \"\n",
    "                                  \"found for image {0}.\"\n",
    "                                  .format(image_full_path))\n",
    "\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def verify_segmentation_dataset(images_path, segs_path,\n",
    "                                n_classes, show_all_errors=False):\n",
    "    try:\n",
    "        img_seg_pairs = get_pairs_from_paths(images_path, segs_path)\n",
    "        if not len(img_seg_pairs):\n",
    "            print(\"Couldn't load any data from images_path: \"\n",
    "                  \"{0} and segmentations path: {1}\"\n",
    "                  .format(images_path, segs_path))\n",
    "            return False\n",
    "\n",
    "        return_value = True\n",
    "        for im_fn, seg_fn in tqdm(img_seg_pairs):\n",
    "            img = cv2.imread(im_fn)\n",
    "            seg = cv2.imread(seg_fn)\n",
    "            # Check dimensions match\n",
    "            if not img.shape == seg.shape:\n",
    "                return_value = False\n",
    "                print(\"The size of image {0} and its segmentation {1} \"\n",
    "                      \"doesn't match (possibly the files are corrupt).\"\n",
    "                      .format(im_fn, seg_fn))\n",
    "                if not show_all_errors:\n",
    "                    break\n",
    "            else:\n",
    "                max_pixel_value = np.max(seg[:, :, 0])\n",
    "                if max_pixel_value >= n_classes:\n",
    "                    return_value = False\n",
    "                    print(\"The pixel values of the segmentation image {0} \"\n",
    "                          \"violating range [0, {1}]. \"\n",
    "                          \"Found maximum pixel value {2}\"\n",
    "                          .format(seg_fn, str(n_classes - 1), max_pixel_value))\n",
    "                    if not show_all_errors:\n",
    "                        break\n",
    "        if return_value:\n",
    "            print(\"Dataset verified! \")\n",
    "        else:\n",
    "            print(\"Dataset not verified!\")\n",
    "        return return_value\n",
    "    except DataLoaderError as e:\n",
    "        print(\"Found error during data loading\\n{0}\".format(str(e)))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path='/Users/mavaylon/Research/image-segmentation-keras/oxford-iiit-pet/images/'\n",
    "segs_path='/Users/mavaylon/Research/image-segmentation-keras/oxford-iiit-pet/annotations/trimaps/'\n",
    "q=get_pairs_from_paths(images_path, segs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=cv2.imread(q[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img_o='/Users/mavaylon/Research/image-segmentation-keras/oxford-iiit-pet/images/Abyssinian_1.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_png_glob=glob('/Users/mavaylon/Research/image-segmentation-keras/oxford-iiit-pet/images/*')\n",
    "jj=glob('/Users/mavaylon/Research/image-segmentation-keras/oxford-iiit-pet/images/*.jpg')\n",
    "jj[0][:-4]+'.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(img_o)\n",
    "im.save('/Users/mavaylon/Research/image-segmentation-keras/images_png/Foto.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for files in jj:\n",
    "    im = Image.open(files)\n",
    "    im.save(files[:-4]+'.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jf=glob('/Users/mavaylon/Research/image-segmentation-keras/images_png/*.png')\n",
    "len(jf)==len(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil  \n",
    "for files in jf:\n",
    "    shutil.move(files,'/Users/mavaylon/Research/image-segmentation-keras/images_png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat=cv2.imread('/Users/mavaylon/Research/image-segmentation-keras/oxford-iiit-pet/annotations/trimaps/Abyssinian_1.png',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob=cv2.imread('/Users/mavaylon/Research/image-segmentation-keras/oxford-iiit-pet/annotations/trimaps/american_pit_bull_terrier_172.png',0)\n",
    "plt.imshow(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo=[]\n",
    "adj=prob-1\n",
    "for i in range(len(adj)):\n",
    "    oo.append(set(adj[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('adj.png',adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op=[]\n",
    "for i in range(len(cat)):\n",
    "    op.append(set(cat[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks=glob('/Users/mavaylon/Research/image-segmentation-keras/oxford-iiit-pet/annotations/trimaps/*')\n",
    "imgs=glob('/Users/mavaylon/Research/image-segmentation-keras/images_png/*')\n",
    "for files in masks:\n",
    "    adj=cv2.imread(files)\n",
    "    adj=adj-1\n",
    "    cv2.imwrite('/Users/mavaylon/Research/image-segmentation-keras/adj_masks/'+files[86:],adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(masks)==len(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len('/Users/mavaylon/Research/image-segmentation-keras/oxford-iiit-pet/annotations/trimaps/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks[55][86:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-contrib-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
