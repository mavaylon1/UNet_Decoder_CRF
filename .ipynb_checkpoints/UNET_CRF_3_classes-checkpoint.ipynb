{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of devices: 1\n",
      "conv4 (None, 16, 16, 1024)\n",
      "conv3 (None, 32, 32, 512)\n",
      "up_ (None, None, None, 512)\n",
      "(None, 32, 32, 1024)\n",
      "(None, 64, 64, 512)\n",
      "(None, 128, 128, 256)\n",
      "(None, 256, 256, 128)\n",
      "(None, 256, 256, 64)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, ZeroPadding2D, \\\n",
    "    Dropout, Conv2DTranspose, Cropping2D, Add, UpSampling2D, BatchNormalization\n",
    "from keras.layers.merge import concatenate\n",
    "from image_segmentation_keras.keras_segmentation.models.model_utils import get_segmentation_model\n",
    "from glob import glob\n",
    "import sys\n",
    "sys.path.insert(1, './src')\n",
    "from crfrnn_model import get_crfrnn_model_def\n",
    "from crfrnn_layer import CrfRnnLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Open a strategy scope\n",
    "i=0\n",
    "if i==0:\n",
    "    input_height = 256\n",
    "    input_width = 256\n",
    "    n_classes = 3\n",
    "    channels = 3\n",
    "    \n",
    "    img_input = Input(shape=(input_height,input_width, channels))\n",
    "\n",
    "    conv0 = Conv2D(64, (3, 3), activation='relu', padding='same')(img_input)\n",
    "#     conv0 = Dropout(0.2)(conv0)\n",
    "    conv0 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv0)\n",
    "    bn0 = BatchNormalization()(conv0)\n",
    "    pool0 = MaxPooling2D((2, 2))(bn0)\n",
    "    \n",
    "    conv1 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool0)\n",
    "#     conv1 = Dropout(0.2)(conv1)\n",
    "    conv1 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    bn1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(bn1)\n",
    "\n",
    "    conv2 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool1)\n",
    "#     conv2 = Dropout(0.2)(conv2)\n",
    "    conv2 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    bn2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(bn2)\n",
    "\n",
    "    conv3 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool2)\n",
    "#     conv3 = Dropout(0.2)(conv3)\n",
    "    conv3 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    bn3 = BatchNormalization()(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(bn3)\n",
    "    \n",
    "    conv4 = Conv2D(1024, (3, 3), activation='relu', padding='same')(pool3)\n",
    "#     conv4 = Dropout(0.2)(conv4)\n",
    "    conv4 = Conv2D(1024, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    print(\"conv4\",conv4.shape)\n",
    "    print('conv3',conv3.shape)\n",
    "\n",
    "    up_= Conv2DTranspose(512,(2,2),strides=2,padding='same')(conv4)\n",
    "    print('up_',up_.shape)\n",
    "    up0 = concatenate([up_, conv3], axis=3)\n",
    "    print(up0.shape)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(up0)\n",
    "#     conv5 = Dropout(0.2)(conv5)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    bn4 = BatchNormalization()(conv5)\n",
    "    \n",
    "    up_2= Conv2DTranspose(256,(2,2),strides=2,padding='same')(bn4)\n",
    "    up1 = concatenate([up_2, conv2], axis=-1)\n",
    "    print(up1.shape)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up1)\n",
    "#     conv6 = Dropout(0.2)(conv6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    bn5 = BatchNormalization()(conv6)\n",
    "    \n",
    "    up_3= Conv2DTranspose(128,(2,2),strides=2,padding='same')(bn5)\n",
    "    up2 = concatenate([up_3, conv1], axis=3)\n",
    "    print(up2.shape)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up2)\n",
    "#     conv7 = Dropout(0.2)(conv7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    bn6 = BatchNormalization()(conv7)\n",
    "    \n",
    "    up_4= Conv2DTranspose(64,(2,2),strides=2,padding='same')(bn6)\n",
    "    up3 = concatenate([up_4, conv0], axis=3)\n",
    "    print(up3.shape)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up3)\n",
    "#     conv8 = Dropout(0.2)(conv8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    bn7 = BatchNormalization()(conv8)\n",
    "    print(conv8.shape)\n",
    "    out = Conv2D( n_classes, (1, 1), activation='relu', padding='same')(bn7)   \n",
    "    crf_output = CrfRnnLayer(image_dims=(input_height, input_width),\n",
    "                         num_classes=n_classes,\n",
    "                         theta_alpha=160.,\n",
    "                         theta_beta=3.,\n",
    "                         theta_gamma=3.,\n",
    "                         num_iterations=10,\n",
    "                         name='crfrnn')([out, img_input])\n",
    "    model = get_segmentation_model(img_input, crf_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5912/5912 [00:20<00:00, 285.03it/s]\n",
      "  0%|          | 0/1478 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset verified! \n",
      "Verifying validation dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1478/1478 [00:05<00:00, 283.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset verified! \n",
      "correct\n",
      "Epoch 1/20\n",
      "5912/5912 [==============================] - 17423s 3s/step - loss: 0.6254 - accuracy: 0.7600 - val_loss: 0.5332 - val_accuracy: 0.6738\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.67382, saving model to pet_class_crf.h5\n",
      "Epoch 2/20\n",
      "5912/5912 [==============================] - 17313s 3s/step - loss: 0.5186 - accuracy: 0.8075 - val_loss: 0.2989 - val_accuracy: 0.8071\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.67382 to 0.80711, saving model to pet_class_crf.h5\n",
      "Epoch 3/20\n",
      "5912/5912 [==============================] - 17306s 3s/step - loss: 0.4656 - accuracy: 0.8299 - val_loss: 0.2796 - val_accuracy: 0.8298\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.80711 to 0.82984, saving model to pet_class_crf.h5\n",
      "Epoch 4/20\n",
      "5912/5912 [==============================] - 17308s 3s/step - loss: 0.4294 - accuracy: 0.8442 - val_loss: 0.2625 - val_accuracy: 0.8357\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.82984 to 0.83567, saving model to pet_class_crf.h5\n",
      "Epoch 5/20\n",
      "5912/5912 [==============================] - 17822s 3s/step - loss: 0.4036 - accuracy: 0.8545 - val_loss: 0.2609 - val_accuracy: 0.8378\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.83567 to 0.83781, saving model to pet_class_crf.h5\n",
      "Epoch 6/20\n",
      "5912/5912 [==============================] - 25738s 4s/step - loss: 0.3833 - accuracy: 0.8622 - val_loss: 0.3474 - val_accuracy: 0.8376\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.83781\n",
      "Epoch 7/20\n",
      "5912/5912 [==============================] - 17141s 3s/step - loss: 0.3653 - accuracy: 0.8689 - val_loss: 0.2763 - val_accuracy: 0.8488\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.83781 to 0.84875, saving model to pet_class_crf.h5\n",
      "Epoch 8/20\n",
      "5912/5912 [==============================] - 17173s 3s/step - loss: 0.3485 - accuracy: 0.8755 - val_loss: 0.2717 - val_accuracy: 0.8498\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.84875 to 0.84979, saving model to pet_class_crf.h5\n",
      "Epoch 9/20\n",
      "5912/5912 [==============================] - 17609s 3s/step - loss: 0.3303 - accuracy: 0.8821 - val_loss: 1.2210 - val_accuracy: 0.7013\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.84979\n",
      "Epoch 10/20\n",
      "5912/5912 [==============================] - 17867s 3s/step - loss: 0.3203 - accuracy: 0.8857 - val_loss: 0.2566 - val_accuracy: 0.8528\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.84979 to 0.85281, saving model to pet_class_crf.h5\n",
      "Epoch 11/20\n",
      "5912/5912 [==============================] - 17433s 3s/step - loss: 0.3113 - accuracy: 0.8890 - val_loss: 0.2779 - val_accuracy: 0.8623\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.85281 to 0.86225, saving model to pet_class_crf.h5\n",
      "Epoch 12/20\n",
      "5912/5912 [==============================] - 17199s 3s/step - loss: 0.2977 - accuracy: 0.8937 - val_loss: 0.2631 - val_accuracy: 0.8199\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.86225\n",
      "Epoch 13/20\n",
      "5912/5912 [==============================] - 17182s 3s/step - loss: 0.2958 - accuracy: 0.8943 - val_loss: 1.0937 - val_accuracy: 0.8556\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.86225\n",
      "Epoch 14/20\n",
      "5912/5912 [==============================] - 17708s 3s/step - loss: 0.2789 - accuracy: 0.9003 - val_loss: 0.4345 - val_accuracy: 0.8262\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.86225\n",
      "Epoch 15/20\n",
      "5912/5912 [==============================] - 17147s 3s/step - loss: 0.2763 - accuracy: 0.9012 - val_loss: 1.1287 - val_accuracy: 0.8382\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.86225\n",
      "Epoch 16/20\n",
      "5912/5912 [==============================] - 17128s 3s/step - loss: 0.2650 - accuracy: 0.9048 - val_loss: 0.2742 - val_accuracy: 0.8613\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.86225\n",
      "Epoch 17/20\n",
      "5912/5912 [==============================] - 17106s 3s/step - loss: 0.2580 - accuracy: 0.9074 - val_loss: 1.1340 - val_accuracy: 0.8438\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.86225\n",
      "Epoch 18/20\n",
      "5912/5912 [==============================] - 17148s 3s/step - loss: 0.2535 - accuracy: 0.9090 - val_loss: 0.2167 - val_accuracy: 0.8688\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.86225 to 0.86875, saving model to pet_class_crf.h5\n",
      "Epoch 19/20\n",
      "5912/5912 [==============================] - 17781s 3s/step - loss: 0.2418 - accuracy: 0.9129 - val_loss: 0.2102 - val_accuracy: 0.8734\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.86875 to 0.87344, saving model to pet_class_crf.h5\n",
      "Epoch 20/20\n",
      "5912/5912 [==============================] - 17335s 3s/step - loss: 0.2325 - accuracy: 0.9158 - val_loss: 1.2561 - val_accuracy: 0.8026\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.87344\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    train_images =  \"/Users/mavaylon/Research/image-segmentation-keras/Data/train/img/\",\n",
    "    train_annotations = \"/Users/mavaylon/Research/image-segmentation-keras/Data/train/ann/\",\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(glob(\"/Users/mavaylon/Research/image-segmentation-keras/Data/train/img/*\")),\n",
    "    batch_size=1,\n",
    "    validate=True,\n",
    "    val_images=\"/Users/mavaylon/Research/image-segmentation-keras/Data/test/img/\",\n",
    "    val_annotations=\"/Users/mavaylon/Research/image-segmentation-keras/Data/test/ann/\",\n",
    "    val_batch_size=1,\n",
    "    val_steps_per_epoch=len(glob(\"/Users/mavaylon/Research/image-segmentation-keras/Data/test/img/*\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
