{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of devices: 1\n",
      "conv4 (None, 16, 16, 1024)\n",
      "conv3 (None, 32, 32, 512)\n",
      "up_ (None, None, None, 512)\n",
      "(None, 32, 32, 1024)\n",
      "(None, 64, 64, 512)\n",
      "(None, 128, 128, 256)\n",
      "(None, 256, 256, 128)\n",
      "(None, 256, 256, 64)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, ZeroPadding2D, \\\n",
    "    Dropout, Conv2DTranspose, Cropping2D, Add, UpSampling2D, BatchNormalization\n",
    "from keras.layers.merge import concatenate\n",
    "from image_segmentation_keras.keras_segmentation.models.model_utils import get_segmentation_model\n",
    "from glob import glob\n",
    "import sys\n",
    "sys.path.insert(1, './src')\n",
    "from crfrnn_model import get_crfrnn_model_def\n",
    "from crfrnn_layer import CrfRnnLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Open a strategy scope\n",
    "i=0\n",
    "if i==0:\n",
    "    input_height = 256\n",
    "    input_width = 256\n",
    "    n_classes = 3\n",
    "    channels = 3\n",
    "    \n",
    "    img_input = Input(shape=(input_height,input_width, channels))\n",
    "\n",
    "    conv0 = Conv2D(64, (3, 3), activation='relu', padding='same')(img_input)\n",
    "#     conv0 = Dropout(0.2)(conv0)\n",
    "    conv0 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv0)\n",
    "    bn0 = BatchNormalization()(conv0)\n",
    "    pool0 = MaxPooling2D((2, 2))(bn0)\n",
    "    \n",
    "    conv1 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool0)\n",
    "#     conv1 = Dropout(0.2)(conv1)\n",
    "    conv1 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    bn1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(bn1)\n",
    "\n",
    "    conv2 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool1)\n",
    "#     conv2 = Dropout(0.2)(conv2)\n",
    "    conv2 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    bn2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(bn2)\n",
    "\n",
    "    conv3 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool2)\n",
    "#     conv3 = Dropout(0.2)(conv3)\n",
    "    conv3 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    bn3 = BatchNormalization()(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(bn3)\n",
    "    \n",
    "    conv4 = Conv2D(1024, (3, 3), activation='relu', padding='same')(pool3)\n",
    "#     conv4 = Dropout(0.2)(conv4)\n",
    "    conv4 = Conv2D(1024, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    print(\"conv4\",conv4.shape)\n",
    "    print('conv3',conv3.shape)\n",
    "\n",
    "    up_= Conv2DTranspose(512,(2,2),strides=2,padding='same')(conv4)\n",
    "    print('up_',up_.shape)\n",
    "    up0 = concatenate([up_, conv3], axis=3)\n",
    "    print(up0.shape)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(up0)\n",
    "#     conv5 = Dropout(0.2)(conv5)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    bn4 = BatchNormalization()(conv5)\n",
    "    \n",
    "    up_2= Conv2DTranspose(256,(2,2),strides=2,padding='same')(bn4)\n",
    "    up1 = concatenate([up_2, conv2], axis=-1)\n",
    "    print(up1.shape)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up1)\n",
    "#     conv6 = Dropout(0.2)(conv6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    bn5 = BatchNormalization()(conv6)\n",
    "    \n",
    "    up_3= Conv2DTranspose(128,(2,2),strides=2,padding='same')(bn5)\n",
    "    up2 = concatenate([up_3, conv1], axis=3)\n",
    "    print(up2.shape)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up2)\n",
    "#     conv7 = Dropout(0.2)(conv7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    bn6 = BatchNormalization()(conv7)\n",
    "    \n",
    "    up_4= Conv2DTranspose(64,(2,2),strides=2,padding='same')(bn6)\n",
    "    up3 = concatenate([up_4, conv0], axis=3)\n",
    "    print(up3.shape)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up3)\n",
    "#     conv8 = Dropout(0.2)(conv8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    bn7 = BatchNormalization()(conv8)\n",
    "    print(conv8.shape)\n",
    "    out = Conv2D( n_classes, (1, 1), activation='relu', padding='same')(bn7)   \n",
    "    crf_output = CrfRnnLayer(image_dims=(input_height, input_width),\n",
    "                         num_classes=n_classes,\n",
    "                         theta_alpha=160.,\n",
    "                         theta_beta=3.,\n",
    "                         theta_gamma=3.,\n",
    "                         num_iterations=10,\n",
    "                         name='crfrnn')([out, img_input])\n",
    "    model = get_segmentation_model(img_input, crf_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5912/5912 [00:20<00:00, 294.89it/s]\n",
      "  2%|▏         | 30/1478 [00:00<00:04, 292.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset verified! \n",
      "Verifying validation dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1478/1478 [00:04<00:00, 305.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset verified! \n",
      "correct\n",
      "Epoch 1/20\n",
      "5912/5912 [==============================] - 17398s 3s/step - loss: 0.6341 - accuracy: 0.7567 - val_loss: 0.4829 - val_accuracy: 0.7899\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.78994, saving model to pet_class_crf.h5\n",
      "Epoch 2/20\n",
      "5912/5912 [==============================] - 17730s 3s/step - loss: 0.5380 - accuracy: 0.8003 - val_loss: 0.4259 - val_accuracy: 0.8136\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.78994 to 0.81358, saving model to pet_class_crf.h5\n",
      "Epoch 3/20\n",
      "5912/5912 [==============================] - 17379s 3s/step - loss: 0.4794 - accuracy: 0.8247 - val_loss: 0.3971 - val_accuracy: 0.8322\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.81358 to 0.83221, saving model to pet_class_crf.h5\n",
      "Epoch 4/20\n",
      "5912/5912 [==============================] - 17397s 3s/step - loss: 0.4509 - accuracy: 0.8364 - val_loss: 0.4502 - val_accuracy: 0.8010\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.83221\n",
      "Epoch 5/20\n",
      "5912/5912 [==============================] - 18456s 3s/step - loss: 0.4071 - accuracy: 0.8537 - val_loss: 0.4164 - val_accuracy: 0.8440\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.83221 to 0.84396, saving model to pet_class_crf.h5\n",
      "Epoch 6/20\n",
      "5912/5912 [==============================] - 18752s 3s/step - loss: 0.3916 - accuracy: 0.8597 - val_loss: 0.4363 - val_accuracy: 0.8435\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.84396\n",
      "Epoch 7/20\n",
      "5912/5912 [==============================] - 17559s 3s/step - loss: 0.3693 - accuracy: 0.8679 - val_loss: 0.3986 - val_accuracy: 0.8506\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.84396 to 0.85062, saving model to pet_class_crf.h5\n",
      "Epoch 8/20\n",
      "5912/5912 [==============================] - 17455s 3s/step - loss: 0.3496 - accuracy: 0.8753 - val_loss: 0.3828 - val_accuracy: 0.8537\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.85062 to 0.85370, saving model to pet_class_crf.h5\n",
      "Epoch 9/20\n",
      "5912/5912 [==============================] - 17467s 3s/step - loss: 0.3400 - accuracy: 0.8787 - val_loss: 0.4065 - val_accuracy: 0.8603\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.85370 to 0.86029, saving model to pet_class_crf.h5\n",
      "Epoch 10/20\n",
      "5912/5912 [==============================] - 18457s 3s/step - loss: 0.3277 - accuracy: 0.8833 - val_loss: 0.4553 - val_accuracy: 0.8506\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.86029\n",
      "Epoch 11/20\n",
      "5912/5912 [==============================] - 17862s 3s/step - loss: 0.3180 - accuracy: 0.8868 - val_loss: 0.4034 - val_accuracy: 0.8616\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.86029 to 0.86157, saving model to pet_class_crf.h5\n",
      "Epoch 12/20\n",
      "5912/5912 [==============================] - 17627s 3s/step - loss: 0.3084 - accuracy: 0.8903 - val_loss: 0.3572 - val_accuracy: 0.8620\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.86157 to 0.86202, saving model to pet_class_crf.h5\n",
      "Epoch 13/20\n",
      "5912/5912 [==============================] - 17494s 3s/step - loss: 0.2951 - accuracy: 0.8949 - val_loss: 0.3778 - val_accuracy: 0.8552\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.86202\n",
      "Epoch 14/20\n",
      "5912/5912 [==============================] - 17669s 3s/step - loss: 0.2840 - accuracy: 0.8987 - val_loss: 0.3877 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.86202\n",
      "Epoch 15/20\n",
      "5912/5912 [==============================] - 19581s 3s/step - loss: 0.2774 - accuracy: 0.9009 - val_loss: 0.3986 - val_accuracy: 0.8613\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.86202\n",
      "Epoch 16/20\n",
      "5912/5912 [==============================] - 18204s 3s/step - loss: 0.2725 - accuracy: 0.9025 - val_loss: 0.3941 - val_accuracy: 0.8749\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.86202 to 0.87488, saving model to pet_class_crf.h5\n",
      "Epoch 17/20\n",
      "5912/5912 [==============================] - 17570s 3s/step - loss: 0.2635 - accuracy: 0.9057 - val_loss: 0.3509 - val_accuracy: 0.8755\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.87488 to 0.87551, saving model to pet_class_crf.h5\n",
      "Epoch 18/20\n",
      "5912/5912 [==============================] - 17504s 3s/step - loss: 0.2631 - accuracy: 0.9059 - val_loss: 0.3838 - val_accuracy: 0.8715\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.87551\n",
      "Epoch 19/20\n",
      "5912/5912 [==============================] - 17624s 3s/step - loss: 0.2426 - accuracy: 0.9128 - val_loss: 0.3626 - val_accuracy: 0.8689\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.87551\n",
      "Epoch 20/20\n",
      "5912/5912 [==============================] - 17505s 3s/step - loss: 0.2441 - accuracy: 0.9124 - val_loss: 0.3480 - val_accuracy: 0.8658\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.87551\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    train_images =  \"/Users/mavaylon/Research/Data1/train/img/\",\n",
    "    train_annotations = \"/Users/mavaylon/Research/Data1/train/ann/\",\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(glob(\"/Users/mavaylon/Research/Data1/train/img/*\")),\n",
    "    batch_size=1,\n",
    "    validate=True,\n",
    "    val_images=\"/Users/mavaylon/Research/Data1/test/img/\",\n",
    "    val_annotations=\"/Users/mavaylon/Research/Data1/test/ann/\",\n",
    "    val_batch_size=1,\n",
    "    val_steps_per_epoch=len(glob(\"/Users/mavaylon/Research/Data1/test/img/*\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('/Users/mavaylon/Research/pet_weights/unet_petcrf/unet__shuffled_pet_class_crf_bn_after_bothconv.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f876cc113d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ann=cv2.imread('/Users/mavaylon/Research/Data1/test/ann/307great_pyrenees_35.png')\n",
    "img=cv2.imread('/Users/mavaylon/Research/Data1/test/img/307great_pyrenees_35.png')\n",
    "plt.imshow(ann*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "img_names = sorted(glob.glob(\"/Users/mavaylon/Downloads/Equalized/*.png\"))\n",
    "\n",
    "for name in img_names:\n",
    "    out_name = \"/Users/mavaylon/Research/Research_Gambier/bp_lrc_sandstone_resnet_crf/\" + name.split('/')[-1]\n",
    "    print(out_name)\n",
    "    out = model.predict_segmentation(inp=name, out_fname=out_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
