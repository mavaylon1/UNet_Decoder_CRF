{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of devices: 1\n",
      "conv4 (None, 16, 16, 1024)\n",
      "conv3 (None, 32, 32, 512)\n",
      "up_ (None, None, None, 512)\n",
      "(None, 32, 32, 1024)\n",
      "(None, 64, 64, 512)\n",
      "(None, 128, 128, 256)\n",
      "(None, 256, 256, 128)\n",
      "(None, 256, 256, 64)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, ZeroPadding2D, \\\n",
    "    Dropout, Conv2DTranspose, Cropping2D, Add, UpSampling2D, BatchNormalization\n",
    "from keras.layers.merge import concatenate\n",
    "from image_segmentation_keras.keras_segmentation.models.model_utils import get_segmentation_model\n",
    "from glob import glob\n",
    "import sys\n",
    "sys.path.insert(1, './src')\n",
    "from crfrnn_model import get_crfrnn_model_def\n",
    "from crfrnn_layer import CrfRnnLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Open a strategy scope\n",
    "i=0\n",
    "if i==0:\n",
    "    input_height = 256\n",
    "    input_width = 256\n",
    "    n_classes = 3\n",
    "    channels = 3\n",
    "    \n",
    "    img_input = Input(shape=(input_height,input_width, channels))\n",
    "\n",
    "    conv0 = Conv2D(64, (3, 3), activation='relu', padding='same')(img_input)\n",
    "#     conv0 = Dropout(0.2)(conv0)\n",
    "    conv0 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv0)\n",
    "    bn0 = BatchNormalization()(conv0)\n",
    "    pool0 = MaxPooling2D((2, 2))(bn0)\n",
    "    \n",
    "    conv1 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool0)\n",
    "#     conv1 = Dropout(0.2)(conv1)\n",
    "    conv1 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    bn1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(bn1)\n",
    "\n",
    "    conv2 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool1)\n",
    "#     conv2 = Dropout(0.2)(conv2)\n",
    "    conv2 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    bn2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(bn2)\n",
    "\n",
    "    conv3 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool2)\n",
    "#     conv3 = Dropout(0.2)(conv3)\n",
    "    conv3 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    bn3 = BatchNormalization()(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(bn3)\n",
    "    \n",
    "    conv4 = Conv2D(1024, (3, 3), activation='relu', padding='same')(pool3)\n",
    "#     conv4 = Dropout(0.2)(conv4)\n",
    "    conv4 = Conv2D(1024, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    print(\"conv4\",conv4.shape)\n",
    "    print('conv3',conv3.shape)\n",
    "\n",
    "    up_= Conv2DTranspose(512,(2,2),strides=2,padding='same')(conv4)\n",
    "    print('up_',up_.shape)\n",
    "    up0 = concatenate([up_, conv3], axis=3)\n",
    "    print(up0.shape)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(up0)\n",
    "#     conv5 = Dropout(0.2)(conv5)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    bn4 = BatchNormalization()(conv5)\n",
    "    \n",
    "    up_2= Conv2DTranspose(256,(2,2),strides=2,padding='same')(bn4)\n",
    "    up1 = concatenate([up_2, conv2], axis=-1)\n",
    "    print(up1.shape)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up1)\n",
    "#     conv6 = Dropout(0.2)(conv6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    bn5 = BatchNormalization()(conv6)\n",
    "    \n",
    "    up_3= Conv2DTranspose(128,(2,2),strides=2,padding='same')(bn5)\n",
    "    up2 = concatenate([up_3, conv1], axis=3)\n",
    "    print(up2.shape)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up2)\n",
    "#     conv7 = Dropout(0.2)(conv7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    bn6 = BatchNormalization()(conv7)\n",
    "    \n",
    "    up_4= Conv2DTranspose(64,(2,2),strides=2,padding='same')(bn6)\n",
    "    up3 = concatenate([up_4, conv0], axis=3)\n",
    "    print(up3.shape)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up3)\n",
    "#     conv8 = Dropout(0.2)(conv8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    bn7 = BatchNormalization()(conv8)\n",
    "    print(conv8.shape)\n",
    "    out = Conv2D( n_classes, (1, 1), activation='relu', padding='same')(bn7)   \n",
    "    crf_output = CrfRnnLayer(image_dims=(input_height, input_width),\n",
    "                         num_classes=n_classes,\n",
    "                         theta_alpha=160.,\n",
    "                         theta_beta=3.,\n",
    "                         theta_gamma=3.,\n",
    "                         num_iterations=10,\n",
    "                         name='crfrnn')([out, img_input])\n",
    "    model = get_segmentation_model(img_input, crf_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5912/5912 [00:20<00:00, 294.89it/s]\n",
      "  2%|▏         | 30/1478 [00:00<00:04, 292.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset verified! \n",
      "Verifying validation dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1478/1478 [00:04<00:00, 305.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset verified! \n",
      "correct\n",
      "Epoch 1/20\n",
      "5912/5912 [==============================] - 17398s 3s/step - loss: 0.6341 - accuracy: 0.7567 - val_loss: 0.4829 - val_accuracy: 0.7899\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.78994, saving model to pet_class_crf.h5\n",
      "Epoch 2/20\n",
      "5912/5912 [==============================] - 17730s 3s/step - loss: 0.5380 - accuracy: 0.8003 - val_loss: 0.4259 - val_accuracy: 0.8136\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.78994 to 0.81358, saving model to pet_class_crf.h5\n",
      "Epoch 3/20\n",
      "5912/5912 [==============================] - 17379s 3s/step - loss: 0.4794 - accuracy: 0.8247 - val_loss: 0.3971 - val_accuracy: 0.8322\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.81358 to 0.83221, saving model to pet_class_crf.h5\n",
      "Epoch 4/20\n",
      "5912/5912 [==============================] - 17397s 3s/step - loss: 0.4509 - accuracy: 0.8364 - val_loss: 0.4502 - val_accuracy: 0.8010\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.83221\n",
      "Epoch 5/20\n",
      "5912/5912 [==============================] - 18456s 3s/step - loss: 0.4071 - accuracy: 0.8537 - val_loss: 0.4164 - val_accuracy: 0.8440\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.83221 to 0.84396, saving model to pet_class_crf.h5\n",
      "Epoch 6/20\n",
      "5912/5912 [==============================] - 18752s 3s/step - loss: 0.3916 - accuracy: 0.8597 - val_loss: 0.4363 - val_accuracy: 0.8435\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.84396\n",
      "Epoch 7/20\n",
      "5912/5912 [==============================] - 17559s 3s/step - loss: 0.3693 - accuracy: 0.8679 - val_loss: 0.3986 - val_accuracy: 0.8506\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.84396 to 0.85062, saving model to pet_class_crf.h5\n",
      "Epoch 8/20\n",
      "5912/5912 [==============================] - 17455s 3s/step - loss: 0.3496 - accuracy: 0.8753 - val_loss: 0.3828 - val_accuracy: 0.8537\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.85062 to 0.85370, saving model to pet_class_crf.h5\n",
      "Epoch 9/20\n",
      "5912/5912 [==============================] - 17467s 3s/step - loss: 0.3400 - accuracy: 0.8787 - val_loss: 0.4065 - val_accuracy: 0.8603\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.85370 to 0.86029, saving model to pet_class_crf.h5\n",
      "Epoch 10/20\n",
      "5912/5912 [==============================] - 18457s 3s/step - loss: 0.3277 - accuracy: 0.8833 - val_loss: 0.4553 - val_accuracy: 0.8506\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.86029\n",
      "Epoch 11/20\n",
      "5912/5912 [==============================] - 17862s 3s/step - loss: 0.3180 - accuracy: 0.8868 - val_loss: 0.4034 - val_accuracy: 0.8616\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.86029 to 0.86157, saving model to pet_class_crf.h5\n",
      "Epoch 12/20\n",
      "5912/5912 [==============================] - 17627s 3s/step - loss: 0.3084 - accuracy: 0.8903 - val_loss: 0.3572 - val_accuracy: 0.8620\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.86157 to 0.86202, saving model to pet_class_crf.h5\n",
      "Epoch 13/20\n",
      "5912/5912 [==============================] - 17494s 3s/step - loss: 0.2951 - accuracy: 0.8949 - val_loss: 0.3778 - val_accuracy: 0.8552\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.86202\n",
      "Epoch 14/20\n",
      "5912/5912 [==============================] - 17669s 3s/step - loss: 0.2840 - accuracy: 0.8987 - val_loss: 0.3877 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.86202\n",
      "Epoch 15/20\n",
      "5912/5912 [==============================] - 19581s 3s/step - loss: 0.2774 - accuracy: 0.9009 - val_loss: 0.3986 - val_accuracy: 0.8613\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.86202\n",
      "Epoch 16/20\n",
      "5912/5912 [==============================] - 18204s 3s/step - loss: 0.2725 - accuracy: 0.9025 - val_loss: 0.3941 - val_accuracy: 0.8749\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.86202 to 0.87488, saving model to pet_class_crf.h5\n",
      "Epoch 17/20\n",
      "5912/5912 [==============================] - 17570s 3s/step - loss: 0.2635 - accuracy: 0.9057 - val_loss: 0.3509 - val_accuracy: 0.8755\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.87488 to 0.87551, saving model to pet_class_crf.h5\n",
      "Epoch 18/20\n",
      "5912/5912 [==============================] - 17504s 3s/step - loss: 0.2631 - accuracy: 0.9059 - val_loss: 0.3838 - val_accuracy: 0.8715\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.87551\n",
      "Epoch 19/20\n",
      "5912/5912 [==============================] - 17624s 3s/step - loss: 0.2426 - accuracy: 0.9128 - val_loss: 0.3626 - val_accuracy: 0.8689\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.87551\n",
      "Epoch 20/20\n",
      "5912/5912 [==============================] - 17505s 3s/step - loss: 0.2441 - accuracy: 0.9124 - val_loss: 0.3480 - val_accuracy: 0.8658\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.87551\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    train_images =  \"/Users/mavaylon/Research/Data1/train/img/\",\n",
    "    train_annotations = \"/Users/mavaylon/Research/Data1/train/ann/\",\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(glob(\"/Users/mavaylon/Research/Data1/train/img/*\")),\n",
    "    batch_size=1,\n",
    "    validate=True,\n",
    "    val_images=\"/Users/mavaylon/Research/Data1/test/img/\",\n",
    "    val_annotations=\"/Users/mavaylon/Research/Data1/test/ann/\",\n",
    "    val_batch_size=1,\n",
    "    val_steps_per_epoch=len(glob(\"/Users/mavaylon/Research/Data1/test/img/*\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
